[{"categories":null,"content":"üññ Re-Creating Star Trek‚Äôs LCARS Computer With Modern LLMs A hands-on experiment in autonomous tool-building agents Imagine walking into your home and saying, ‚ÄúComputer, show me when I last watered the plants and remind me to buy groceries.‚Äù No app switching, no hunting through interfaces, no remembering which service stores what data. Just a conversation with your digital assistant that understands context, remembers everything, and can act on your behalf. This isn‚Äôt science fiction anymore‚Äîit‚Äôs Tuesday afternoon in my living room. What started as a quest to recreate Star Trek‚Äôs iconic LCARS computer has become something far more practical: a universal replacement for the dozens of specialized apps cluttering our digital lives. Like a Swiss Army knife for the smartphone age, this system demonstrates how the marriage of large language models and tool-calling APIs can transform the way we interact with technology. But the journey from starship computer to everyday assistant revealed something unexpected: the most powerful sci-fi technologies aren‚Äôt the ones that dazzle with complexity, but those that disappear into the background, becoming as natural as conversation itself. ","date":"2025-07-08","objectID":"/posts/computer/:1:0","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"1 ¬∑ Why LCARS? For decades science-fiction fans have watched Star Trek officers converse with a ship-wide computer LCARS (‚ÄúLibrary Computer Access/Retrieval System‚Äù). LCARS could: answer arbitrary questions by searching internal databases; execute real-world actions by routing commands to subsystems; politely say why it couldn‚Äôt comply when limits were hit. In 2025, large language models (LLMs) plus tool-calling APIs make that dream feel within reach. I decided to find out how close we can get‚Äîarmed with Replit dev env, and a Postgres instance. You can find the open source code here: https://github.com/saftacatalinmihai/Computer ","date":"2025-07-08","objectID":"/posts/computer/:1:1","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"2 ¬∑ Beyond Science Fiction: Replacing Everyday Apps ‚ÄúComputer, log wet diaper change for Stefan at 14:30.‚Äù ‚Äî Me, talking to my watch instead of opening a baby tracker app While building this system, I discovered something unexpected: it‚Äôs already replacing many specialized apps in my daily life. Like a universal translator that breaks down language barriers, this architecture dissolves the barriers between apps, creating a unified interface for all your digital needs. Think of it as the difference between having a Swiss Army knife versus carrying a separate tool for every task. The same conversational interface that lets LCARS control starship functions works surprisingly well for mundane tasks that would normally require dedicated applications. I find that it already can replace many apps like note-taking, journaling, baby tracker, food inventory‚Ä¶ simple apps that are just CRUD apps for writing and reading from a database. It‚Äôs like having a personal digital butler who knows where you keep everything and can fetch it on command. Consider how many apps on your phone are essentially just fancy interfaces for: Creating records Reading information back Updating existing data Deleting things you no longer need This is known as CRUD - the four basic operations of persistent storage. Each app is like a specialized shopkeeper in a digital mall, each with its own storefront, its own rules, and its own way of organizing inventory. Each with its own account, learning curve, subscription fee, and data silo. What if one conversational interface could replace them all? What if, instead of visiting dozens of specialized shops, you could speak to one knowledgeable concierge who had access to everything? Captain's Log, Supplemental: The irony isn't lost on me that in trying to recreate sci-fi technology from the future, I've ended up replacing dozens of present-day apps with something that feels both more advanced and more intuitive. ","date":"2025-07-08","objectID":"/posts/computer/:1:2","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"3 ¬∑ Architecture at a Glance Like the Enterprise itself, this system is built on a modular architecture where each component serves a specific purpose, but they all work together seamlessly. Think of it as a digital starship where each deck has its own function, but they‚Äôre all connected by the same communication network. Layer Purpose Starship Analogy LLM Clients Natural-language ‚Äúbrain‚Äù The ship‚Äôs computer core with multiple backup systems Assistant Core Conversation management The bridge crew, coordinating all operations Tool Hub Concrete implementations Engineering and science departments, each with specialized skills Database Knowledge store The ship‚Äôs library and data banks Interfaces User interaction Communication channels: viewscreen, comm badges, and terminals Self-Modification System evolution The ship‚Äôs ability to adapt and upgrade its own systems This architecture is specifically designed to enable flexible CRUD operations through natural language. The database layer supports dynamic schema creation, while the LLM ‚Äúbrain‚Äù translates user intentions into appropriate database operations‚Äîmuch like how the universal translator converts alien languages into understandable speech. graph TD; User[User] --\u003e |Natural Language| Interfaces subgraph Interfaces CLI[Command-Line REPL] Web[Web Interface] Slack[Slack Bot] end Interfaces --\u003e |Requests| Assistant subgraph Assistant Core V1[Assistant v1] V2[Assistant v2] V3[Assistant v3] V4[Assistant v4] Loader[Assistant Loader] end Assistant --\u003e |Queries| LLM[LLM Clients] LLM --\u003e |Responses| Assistant Assistant --\u003e |Tool Calls| Tools subgraph Tool Hub SQL[SQL Operations] Python[Python Execution] ASCII[ASCII Art Generation] Self[Self-Modification] end Tools --\u003e |Results| Assistant SQL --\u003e DB subgraph Database SQLite[SQLite] Postgres[PostgreSQL] end ","date":"2025-07-08","objectID":"/posts/computer/:1:3","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"4 ¬∑ How Tools Work Tools in this system work like skilled crew members on a starship‚Äîeach one has a specific expertise, but they all speak the same language and follow the same protocols. When Captain Picard says ‚ÄúComputer, show me all Federation ships in the Neutral Zone,‚Äù the computer doesn‚Äôt just magically know what to do. It breaks down the request, identifies the right databases to query, and formats the response appropriately. Our system works similarly, but for everyday tasks. Tool Implementation Each tool is a Python function in the tools/ directory. The SQL tool is the heart of the CRUD functionality‚Äîthink of it as the ship‚Äôs librarian who knows exactly where every piece of information is stored and how to retrieve it: # From tools/sql.py def run_sql(conn): def inner(sql_statements: str): column_names = [] rows = [] parsed_sql_list = parse_sql(str(sql_statements)) for sql_statement in parsed_sql_list: if not sql_statement.strip(): continue cursor = conn.cursor() cursor.execute(sql_statement) if cursor.description: column_names = [description[0] for description in cursor.description] rows = cursor.fetchall() else: column_names = [] rows = [] conn.commit() # Format and return results... return inner System Prompt for SQL Tool The magic happens in the system prompt that guides the LLM‚Äîlike Starfleet‚Äôs Prime Directive, these instructions shape how the AI behaves in every interaction. Just as officers receive detailed briefings before away missions, the LLM gets comprehensive instructions on how to handle database operations: Tool: postgres_sql_run Description: Runs SQL against a Postgres database. Instructions: - You interact with a user who is not familiar with SQL. - Generate SQL code based on user requests. Keep it simple and use only Postgres-compatible features. - No multi-tenancy, user access control, complex data validation, or error handling beyond basic SQL. - For relative time (e.g., \"yesterday\"), use Postgres's relative time functions. - DO NOT insert sample or placeholder data. Only use data mentioned by the user. - After any DDL (CREATE, ALTER, DROP) or DML (INSERT, UPDATE, DELETE) statement, append a `SELECT * FROM table_name;` query for the modified table to show its new state. - Assume tables might contain data, but you don't know what it is. Your role is to generate SQL. - For querying, aim for broad matches (e.g., case-insensitive search if appropriate for the user's request). - Provide only the final, executable SQL code. No hypothetical examples. - If the schema seems insufficient for the user's request, include `CREATE TABLE` statements first, then the necessary DML. - The existing Postgres SQL Schema is: [CURRENT SCHEMA INJECTED HERE] This prompt is critical because it explicitly allows the LLM to create new tables and modify schemas when needed. It‚Äôs like giving the ship‚Äôs computer permission to create new database categories when it encounters previously unknown phenomena. The last instruction is particularly powerful: ‚ÄúIf the schema seems insufficient for the user‚Äôs request, include CREATE TABLE statements first‚Ä¶‚Äù This is where the system transcends traditional apps. Instead of being constrained by a fixed schema designed by someone else, the database grows organically like a living organism, adapting to your actual needs. Tool Schema Definition The LLM needs to know how to call the tool, which is defined in a schema: { \"type\": \"function\", \"function\": { \"name\": \"postgres_sql_run\", \"description\": \"Runs one or more SQL statements against a Postgres database and returns the result of the last query.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"sql_statements\": { \"type\": \"string\", \"description\": \"A single string containing one or more SQL statements separated by semicolons.\" } }, \"required\": [\"sql_statements\"] } } } Call Flow with Schema Evolution When a user makes a request that requires new data storage, the system orchestrates a sophisticated dance between understanding, planning, and execu","date":"2025-07-08","objectID":"/posts/computer/:1:4","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"5 ¬∑ LLM Prompt Engineering The system prompt is dynamically constructed with: Persona Definition: Establishes the assistant‚Äôs identity and capabilities Tool Descriptions: Detailed information about available tools and their parameters Database Schema: When using database tools, the schema is included for context Conversation History: Previous interactions to maintain context Special Instructions: Custom behavior guidelines based on the assistant version The prompt construction is version-specific, with each assistant version (v1-v4) having its own prompt engineering approach. This allows the system to evolve its interaction style and capabilities over time. For CRUD operations, including the current database schema in the prompt is essential. This gives the LLM awareness of existing tables and fields, allowing it to generate appropriate SQL for creating, reading, updating, or deleting records based on conversational requests. ","date":"2025-07-08","objectID":"/posts/computer/:1:5","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"6 ¬∑ Self-Modification Capabilities In the Star Trek universe, the most fascinating aspect of advanced computers isn‚Äôt their computational power‚Äîit‚Äôs their ability to learn, adapt, and evolve. Like Data‚Äôs neural networks creating new pathways based on experience, this system can modify its own code and capabilities. The system can modify itself through a sophisticated self-awareness mechanism that would make any Federation engineer proud: Code Reading The assistant can read its own source code: # Helper function to get current assistant code def get_current_assistant_code(file_path): \"\"\"Reads and returns the content of the current assistant.py file.\"\"\" try: with open(file_path, 'r', encoding='utf-8') as f: return f.read() except Exception as e: print( f\"Critical Error: Could not read own source code at {file_path}. Error: {e}\" ) return \"# Error: Could not read assistant code. Self-update features will be impaired.\" Version Management When creating a new version, the system: Reads its current implementation Makes the requested modifications Saves as a new version with incremented number (e.g., assistant_v4.py) Updates the assistant loader to recognize the new version Tool Addition New tools are added by: Creating a new tool file in the tools/ directory Updating the assistant code to include the tool in its mapping Defining the tool‚Äôs schema for the LLM This is implemented through special placeholders in the code: #\u003cTOOL_IMPORT\u003e - Where to add new tool imports #\u003cTOOL_MAPPING\u003e - Where to add new tool mappings #\u003ctool_schema\u003e - Where to add new tool schemas This self-modification capability is what makes the system truly extensible for CRUD app replacement. When I need a new type of app functionality, I can simply ask the system to create a new tool that handles that specific use case, rather than downloading yet another specialized app. \"Working.\" - The Computer, [Star Trek: The Next Generation](https://en.wikipedia.org/wiki/Star_Trek:_The_Next_Generation) ","date":"2025-07-08","objectID":"/posts/computer/:1:6","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"7 ¬∑ Multiple Interfaces The system provides three distinct interfaces: Command-Line REPL (bot_repl.py) # Interactive command-line interface while True: user_input = input(\"\u003e \") if user_input.lower() in [\"exit\", \"quit\"]: break response = assistant.process_message(user_input) print(response) Web Interface (main.py) A Flask-based web application and API with: User authentication Conversation history Streaming responses Slack Bot (bot_slack.py) Integration with Slack for team communication: Responds to direct messages Handles slash commands Supports threaded conversations Having multiple interfaces means I can interact with my CRUD functionality from wherever is most convenient: Terminal when I‚Äôm already working in the command line Web interface from any device with a browser Slack when collaborating with family or team members This multi-interface approach eliminates one of the biggest limitations of traditional apps: being locked into a single access method. ","date":"2025-07-08","objectID":"/posts/computer/:1:7","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"8 ¬∑ Evolution Through Versions The system has evolved through multiple versions: Version Key Features Added v1 Basic conversation, SQLite support v2 Tool execution, Python code evaluation v3 Self-modification, improved error handling v4 PostgreSQL support, ASCII art generation, enhanced self-modification Each version builds upon the previous one, with the assistant_loader.py module dynamically loading the appropriate version based on configuration or user request. This versioning approach allows: Stable reference points for development Ability to roll back if issues occur when new tools are added by the assistant and it breaks something. Clear tracking of capability evolution Testing new features without breaking existing functionality The progression from SQLite to PostgreSQL in particular has been crucial for CRUD app replacement, as it provides a stable DB in a managed cloud needed for daily from multiple entry points. ","date":"2025-07-08","objectID":"/posts/computer/:1:8","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"9 ¬∑ Real-World Applications: The CRUD App Replacement Showcase The true test of any technology isn‚Äôt in the laboratory‚Äîit‚Äôs in the messy, unpredictable reality of daily life. Like the Enterprise crew facing unexpected challenges that require creative solutions, this system has proven its worth in scenarios that would stump traditional apps. Beyond the technical implementation, this system has proven immediately useful for everyday tasks that benefit from structured data storage with natural language access. It‚Äôs like having a skilled personal assistant who never forgets, never gets tired, and always knows exactly where to find what you need. App Replacement Showcase Consider the digital transformation in your pocket. Where once you might have juggled multiple apps like a circus performer keeping plates spinning, now you have a single, conversational interface that understands context and remembers everything. Traditional App LCARS Implementation Key Advantages Note-taking App ‚ÄúComputer, note that I need to buy milk‚Äù - Natural language input- No app switching- Automatic organization Baby Tracker ‚ÄúLog diaper change for Stefan‚Äù - Shared family access- Custom queries- No subscription fees Food Inventory ‚ÄúAdd milk to fridge, expires July 15‚Äù - Voice input from anywhere- Expiration alerts- Shopping list integration Journaling App ‚ÄúComputer, journal entry: today I‚Ä¶‚Äù - Multi-modal entries- Searchable by content- Integrated with other data Habit Tracker ‚ÄúRecord 30 minutes of meditation‚Äù - Natural reminders- Pattern analysis- No rigid templates 1. Personal Activity Logging It‚Äôs like having a digital memory palace that never forgets. Every interaction becomes a breadcrumb in a trail of personal data that you can query naturally: Plant Care Tracking: Recording when plants were watered, with the ability to later ask ‚ÄúWhen did I last water the plants?‚Äù Habit Tracking: Logging exercise, medication, or other recurring activities with natural language queries 2. Baby Care Assistant New parents know the exhaustion of trying to remember feeding schedules, diaper changes, and sleep patterns. This system becomes like a wise grandmother who remembers everything and offers gentle reminders: Replacing specialized apps with simple natural language commands: \u003e Recording wet diaper change for Stefan \u003e Starting sleep time for Stefan \u003e Stefan just finished feeding for 15 minutes Later querying patterns: ‚ÄúHow many wet diapers today?‚Äù or ‚ÄúWhen was Stefan‚Äôs last feeding?‚Äù 3. Household Management Like the ship‚Äôs computer tracking the status of every system on the Enterprise, this system can monitor the countless details of home management: Inventory Tracking: ‚ÄúAdding milk to fridge, expires on July 15th‚Äù Maintenance Records: ‚ÄúJust changed the furnace filter‚Äù with later queries like ‚ÄúWhen was the furnace filter last changed?‚Äù The system‚Äôs key advantages in these scenarios mirror the benefits of having a ship‚Äôs computer that knows every system, every crew member, and every mission detail: Dynamic Schema Evolution: Like the ship‚Äôs computer adapting to new alien technologies, the assistant can create new tables or modify existing ones based on conversation context Database Transparency: Users can directly access the PostgreSQL database with standard clients for advanced queries‚Äîlike having direct access to the ship‚Äôs data banks when needed Device Integration: Using the Web API with Apple Shortcuts enables voice interaction from any Apple device, including Apple Watch‚Äîyour personal communicator badge Conversational Interface: Natural language input/output eliminates the need to learn specialized app interfaces‚Äîspeak to your computer the way you‚Äôd speak to a colleague This practical utility demonstrates how the technical capabilities translate into real value for users, bridging the gap between science fiction and everyday convenience. It‚Äôs the difference between having a tool that works for you versus having to work for your tools. Captain's Log: It's remarkable h","date":"2025-07-08","objectID":"/posts/computer/:1:9","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"10 ¬∑ Why One System Beats Many Apps In the original Star Trek series, the ship‚Äôs computer wasn‚Äôt a collection of separate programs‚Äîit was a unified intelligence that could connect information across all ship systems. This LCARS-inspired approach offers several advantages that mirror the benefits of having a single, omniscient digital assistant rather than a collection of specialized servants: Unified Data Store: All information lives in one database, enabling cross-domain queries like ‚ÄúShow me all the days I exercised but didn‚Äôt sleep well‚Äù‚Äîconnections that would be impossible when your fitness tracker and sleep app can‚Äôt talk to each other Consistent Interface: Learn one interaction pattern that works across all use cases, rather than navigating different UIs for each app. It‚Äôs like having a universal translator for your digital life No Context Switching: Eliminate the cognitive load of jumping between apps‚Äîjust talk to your computer naturally, like having a conversation with a knowledgeable friend who remembers everything Evolving Functionality: The system grows with your needs through self-modification, rather than waiting for app developers to add features. It‚Äôs like having a technology that adapts to you, rather than requiring you to adapt to it Privacy Control: Your data stays on your infrastructure, not spread across multiple third-party services like digital breadcrumbs scattered across the internet Cost Efficiency: Replace multiple subscription services with a single self-hosted solution‚Äîlike consolidating dozens of monthly bills into one Custom Workflows: Create personalized workflows that span traditional app boundaries, like a digital butler who understands your unique preferences and routines The most surprising benefit has been how the unified approach reveals connections between previously siloed information. It‚Äôs like switching from a flashlight that illuminates one small area to stadium lights that show the entire field. When your exercise data, sleep tracking, and work productivity all live in the same system, patterns emerge that would be invisible when using separate apps. ","date":"2025-07-08","objectID":"/posts/computer/:1:10","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"11 ¬∑ Challenges and Next Steps Current Challenges Security: Python code execution presents significant security risks Error Handling: Improving error recovery and user-friendly error messages Memory Management: Optimizing conversation history for long interactions Tool Dependencies: Managing dependencies between tools and assistant versions Schema Management: Balancing flexibility with consistency in database structure Planned Improvements Dynamic MCP server integration Looking up MCP servers and installing them as tools to be used by the assistant. Multi-Modal Capabilities: Adding image and audio processing Improved Tool Management: More robust tool versioning and dependency tracking Security Hardening: Sandboxed execution environments and permission models Advanced Visualization: Beyond ASCII art to rich interactive visualizations Mobile-First Interfaces: Optimized experiences for on-the-go CRUD operations Community Tool Repository: Sharing specialized tools for different CRUD app replacements Future CRUD App Replacements Health Tracking: Comprehensive wellness monitoring beyond single-purpose health apps Project Management: Replacing task management apps with natural language project coordination Learning Systems: Personal knowledge management and spaced repetition learning IoT Integration: Connecting Internet of Things devices to the CRUD ecosystem Financial Tracking: Personal finance management through conversation \"There are always possibilities.\" - [Spock](https://en.wikipedia.org/wiki/Spock) ","date":"2025-07-08","objectID":"/posts/computer/:1:11","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"‚òëÔ∏è Takeaways Building a real-world LCARS system has taught me that the most powerful sci-fi technologies aren‚Äôt the ones that dazzle with impossible physics‚Äîthey‚Äôre the ones that solve fundamental human problems. The tricorder was revolutionary not because it used exotic particles, but because it put all the information a person might need into a single, intuitive interface. Self-modifying AI assistants are now achievable with modern LLMs and tool-calling APIs, bringing us closer to the adaptive intelligence we‚Äôve dreamed of Multiple interfaces (CLI, Web, Slack) provide flexible access points for different use cases‚Äîlike having multiple ways to contact the ship‚Äôs computer Versioning both the assistant and its tools creates a stable evolution path, allowing the system to grow without breaking Database operations (SQLite/PostgreSQL), Python execution, and visualization capabilities form a powerful foundation for replacing entire categories of apps Self-modification requires careful design with placeholders and version management, but enables unprecedented adaptability CRUD app replacement demonstrates immediate practical utility beyond technical novelty‚Äîthis isn‚Äôt just a proof of concept, it‚Äôs a daily driver Unified data approach reveals insights impossible with siloed app ecosystems, like having a conversation with your data instead of interrogating it The system demonstrates a practical implementation of the LCARS vision from Star Trek, proving that some sci-fi dreams are closer than we think This project shows how close we‚Äôve come to the science fiction dream of conversational computers that can extend their own capabilities‚Äîwhile highlighting the engineering challenges that remain in making such systems robust, secure, and truly autonomous. More surprisingly, it demonstrates how quickly such systems can replace dozens of specialized apps with a single, more flexible interface. The future isn‚Äôt just about having smarter computers‚Äîit‚Äôs about having computers that are easier to talk to. And sometimes, the most futuristic technology is the one that feels the most natural. ‚Äî End log. ** Part of this blog post was written with the help (not by) AI (LLMs). ","date":"2025-07-08","objectID":"/posts/computer/:2:0","tags":null,"title":"Computer","uri":"/posts/computer/"},{"categories":null,"content":"Versiune 1 https://mapy.cz/s/fezafefumu ","date":"2024-08-03","objectID":"/routes/piatramare/:1:0","tags":null,"title":"Piatra Mare 4 Aug 2024","uri":"/routes/piatramare/"},{"categories":null,"content":"Versiune 2 alt: https://mapy.cz/s/jekecunabe ","date":"2024-08-03","objectID":"/routes/piatramare/:2:0","tags":null,"title":"Piatra Mare 4 Aug 2024","uri":"/routes/piatramare/"},{"categories":null,"content":"Add I‚Äôve added a new tldraw tool called Stream Component (left-most icon: \u003c/\u003e in the toolbar, shortcut key S). You can click or draw a rectangle to add a stream component to the canvas. The component is not initialized at first - you need to select the component type from the drop-down menu. This UI will be improved to allow more discoverability, show descriptions of components, etc. After selecting the component type, it will be initialized (started in the backend) and begin running immediately, waiting for inputs on its input port(s). There is no separate step for starting the component; it is always running. I want the designer to have the feeling of working with regular drawings - so normal actions like moving, resizing, deleting, copying, pasting, undo, redo, etc., should work as expected. ","date":"2024-07-10","objectID":"/posts/vsps/:1:0","tags":["scala","streaming","visual_programming"],"title":"Visual Stream Processing System","uri":"/posts/vsps/"},{"categories":null,"content":"Connect You can connect two components by adding arrows from an output port to an input port using regular tldraw arrows. ","date":"2024-07-10","objectID":"/posts/vsps/:2:0","tags":["scala","streaming","visual_programming"],"title":"Visual Stream Processing System","uri":"/posts/vsps/"},{"categories":null,"content":"Type check on connect The backend where the streaming components are started is a Scala program1. This means that the connections between components can be type-checked. If the connection can be made, the arrow turns green; otherwise, it turns red. This can be an advantage over other visual programming systems that are usually dynamically typed. The system still allows the connection to be made, but you can expect a runtime error if the types are not compatible. I chose to allow a more dynamic feel and enable the designer to do whatever they want (allowing rough scribbles or scrappy fiddles), even if that means entering a temporary state where the types don‚Äôt match. It is up to the designer to fix this later. ","date":"2024-07-10","objectID":"/posts/vsps/:2:1","tags":["scala","streaming","visual_programming"],"title":"Visual Stream Processing System","uri":"/posts/vsps/"},{"categories":null,"content":"Edit The component‚Äôs code can be edited in any text editor. I want to make it easy for the programmer to open the code directly from the canvas, so I added a context button that opens the code in a default editor. Currently, changing the code requires a restart of the backend - that is not ideal and will be improved. However, the restart only takes a few seconds, so it is not a significant issue for now. ","date":"2024-07-10","objectID":"/posts/vsps/:3:0","tags":["scala","streaming","visual_programming"],"title":"Visual Stream Processing System","uri":"/posts/vsps/"},{"categories":null,"content":"Logs Instead of adding a custom view for showing logs, I decided to use an existing solution (ELK stack - ElasticSearch, Logstash, Kibana) and embed it in the tool. After all, tldraw allows embedding of any web content with iframes. ","date":"2024-07-10","objectID":"/posts/vsps/:4:0","tags":["scala","streaming","visual_programming"],"title":"Visual Stream Processing System","uri":"/posts/vsps/"},{"categories":null,"content":"Proof of concept: Fibonacci This is a simple example of a Fibonacci stream component. It generates the Fibonacci sequence and prints it. ","date":"2024-07-10","objectID":"/posts/vsps/:5:0","tags":["scala","streaming","visual_programming"],"title":"Visual Stream Processing System","uri":"/posts/vsps/"},{"categories":null,"content":"What‚Äôs the big idea, anyway? This image depicts how much of software is built today. Someone - architect, designer, senior dev - starts by drawing some diagrams. These are representations of what the code for an application should do, not the actual code. Then developers take those diagrams and start implementing code. Most of the time, the diagrams become outdated as new information comes along. What if the diagram can start as a sketch, but can be gradually ‚Äúfilled out‚Äù with the actual implemented code? And at the same time, the diagram can be run as a program. This is the idea behind Visual Stream Processing System (VSPS). I‚Äôm trying to capture a feeling I have when developing streaming applications. I usually start with a high-level component graph (using Pekko Graph DSL). The components are just mocks - only the types are defined, but the graph actually runs. Then I start implementing the components one by one, slowly filling out the graph with actual code, and refactoring the shape as I gather new information about what I actually need to build. The implementation also guides the graph design by surfacing some paths I haven‚Äôt thought of before - especially error cases, edge cases, etc. But I always have a running system, even if it is not fully implemented. At the same time I want to combine this with the feeling of freedom and speed of drawing diagrams like I do for my day job as a software architect. I work a lot with DrawIO and I like having the ability to quickly iterate, duplicate parts of the diagram, mix and match various ideas to show competing options for designs. I want to be able to do the same but with real, running software. This is a highly experimental project that I‚Äôve been working on in my (very limited) spare time, but I think it has potential. If you are interested in this project, please reach out to me. I would love to hear your thoughts on it. Pekko Streams to be precise¬†‚Ü©Ô∏é ","date":"2024-07-10","objectID":"/posts/vsps/:6:0","tags":["scala","streaming","visual_programming"],"title":"Visual Stream Processing System","uri":"/posts/vsps/"},{"categories":null,"content":"Whether you‚Äôre a seasoned developer or just starting your coding journey, these seven talks are packed with insights that will challenge and inspire you. In the rapidly evolving world of software development, it‚Äôs easy to get lost in the sea of new languages, frameworks, and techniques. But as I journeyed through my career as a developer, I found that revisiting seminal talks from industry visionaries consistently provides me with fresh perspectives and enduring wisdom. I compiled this list, initially for myself but now for all of you, to share these invaluable resources that continue to shape my work. If you‚Äôre a developer eager to broaden your horizons or someone curious about the minds that pioneered our field, these seven talks are for you. ","date":"2023-07-09","objectID":"/posts/talks-every-developer-should-watch/:0:0","tags":null,"title":"7 Essential Tech Talks Every Developer Should Watch","uri":"/posts/talks-every-developer-should-watch/"},{"categories":null,"content":"1. Bret Victor - The Future of Programming (2013) The most dangerous taught that you can have as a creative person is to think that you know what you're doing. Because once you think you know what you're doing, you stop looking around for other ways of doing things. Bret Victor is a visionary interface designer and computer scientist known for pushing the boundaries of interactive mediums. He founded the Dynamicland research lab and has contributed to products like the Apple Watch. I have 2 of Bret‚Äôs talks on this list - that shows how much value I get from his talks. I would, of course, recommend all of them. You can find more on worrydream. This first talk is a bit of a history lesson in past ideas on hardware and software. It‚Äôs a good introduction for the following talks. The next talk is mentioned in this one. It‚Äôs the full demo of NSL. ","date":"2023-07-09","objectID":"/posts/talks-every-developer-should-watch/:1:0","tags":null,"title":"7 Essential Tech Talks Every Developer Should Watch","uri":"/posts/talks-every-developer-should-watch/"},{"categories":null,"content":"2. Douglas Engelbart - The Mother of All Demos (1968) If in your office, you as an intellectual worker were supplied with a computer display backed up by a computer that was alive for you all day and was instantly responsive to every action you had, how much value can you derive from that? Douglas Engelbart was a computer scientist who pioneered many of the concepts and technologies that we use today. He is best known for this 1968 demonstration, dubbed ‚ÄúThe Mother of All Demos‚Äù, where he showcased the first computer mouse, video conferencing, teleconferencing, hypertext, word processing, hypermedia, object addressing and dynamic file linking, bootstrapping, and a collaborative real-time editor. This talk may seem show paced to watch at times, for instance showing how to reorder lists of text. Things that are generally well-known and easy to do today. But you have to remember that these things did not exist at that time‚Ä¶ He had to invent it all. Imagine a world where there were no computer mice, no video conferencing, and no word processing‚ÄîEngelbart didn‚Äôt just imagine it, he changed it! Engelbart‚Äôs work was driven by his goal of augmenting human intellect and solving complex problems. He believed that computers could be used as tools to enhance human capabilities and facilitate collective intelligence. He founded the Augmentation Research Center (ARC) at Stanford Research Institute (SRI), where he led a team of researchers and engineers to develop innovative systems and software. He inspired generations of computer scientists and innovators including the author of the next talk: Alan Kay ","date":"2023-07-09","objectID":"/posts/talks-every-developer-should-watch/:2:0","tags":null,"title":"7 Essential Tech Talks Every Developer Should Watch","uri":"/posts/talks-every-developer-should-watch/"},{"categories":null,"content":"3. Alan Kay - The computer revolution hasn‚Äôt happened yet (OOPSLA 1997) I invented the term Object Oriented and I can tell you I didn't have C++ in mind Alan Kay is a pioneering figure in the field of computer science, best known for his groundbreaking work in object-oriented programming and graphical user interfaces. Kay‚Äôs innovative thinking has shaped modern computing as we know it today. Kay joined Xerox PARC (Palo Alto Research Center) in the 1970s. It was here where he made several of his most influential contributions to the field. His vision of a ‚Äúpersonal computer‚Äù - an idea encapsulated in his famous Dynabook concept - was transformation and prophetic. Kay was one of the key architects of the Smalltalk programming language, the first truly object-oriented language. His work in developing graphical user interfaces has underpinned their ubiquitous presence in modern computing, from desktops to smartphones. In 2003, Kay received the prestigious Turing Award, often considered the ‚ÄúNobel Prize of Computing,‚Äù for pioneering many of the ideas at the root of contemporary object-oriented programming languages, leading the team that developed Smalltalk, and for fundamental contributions to personal computing. In this talk, Kay challenged the audience to rethink their understanding of computers and technology, arguing that the true potential of computing was far from being realized. ","date":"2023-07-09","objectID":"/posts/talks-every-developer-should-watch/:3:0","tags":null,"title":"7 Essential Tech Talks Every Developer Should Watch","uri":"/posts/talks-every-developer-should-watch/"},{"categories":null,"content":"4. Joe Armstrong - The Mess We‚Äôre In (2014) - I think software is actually getting worse and worse and worse with time. It's not that we can't do amazing things with computers. We can. But when they don't work, we can't understand why it did. - You shouldn't write systems that violate laws of physics Joe Armstrong was a pioneer in software engineering who created the Erlang programming language, a concurrent, functional, and fault-tolerant language that powers many distributed systems today. He was motivated by the need to build reliable and scalable telecom applications at Ericsson that could handle millions of users without downtime. He also developed a design methodology and a set of libraries called OTP that provide common patterns and tools for building robust systems with Erlang. In this talk, Joe asks what went wrong with programming, given that hardware has improved so much in the past half century, yet is seems that software is a lot slower than it should be just given CPU speedup in MHz and number of processors, and also breaks all the time. ","date":"2023-07-09","objectID":"/posts/talks-every-developer-should-watch/:4:0","tags":null,"title":"7 Essential Tech Talks Every Developer Should Watch","uri":"/posts/talks-every-developer-should-watch/"},{"categories":null,"content":"5. Rich Hickey - Simple Made Easy (2011) Simplicity is a choice. It's your fault if you don't have a simple system. And I think we have a culture of complexity. To the extent we all continue to use these tools that have complex outputs, we're just in a rut. Rich Hickey is a prominent figure in the software engineering world, known for creating the Clojure programming language and the Datomic database system. He has over 20 years of experience in various domains, such as scheduling, broadcast automation, audio analysis, database design, and machine learning. He is also a prolific speaker and writer, who has delivered influential talks and papers on topics such as functional programming, concurrency, data structures, and software design. In this talk, Rich argues that we should pursue simplicity, not ease of use, in software design and avoid complexity by choosing simple tools and constructs and carefully separating concerns. ","date":"2023-07-09","objectID":"/posts/talks-every-developer-should-watch/:5:0","tags":null,"title":"7 Essential Tech Talks Every Developer Should Watch","uri":"/posts/talks-every-developer-should-watch/"},{"categories":null,"content":"6. Stephen Wolfram - A New Kind of Science Even with a definite underlying laws there can still be no effective way to predict what a system will do except in effect just by running the system and seeing what happens. Stephen Wolfram is a renowned computer scientist, physicist, and entrepreneur known for his work in cellular automata, complexity theory, and computational knowledge. He is the creator of Mathematica, a comprehensive computational software, and Wolfram Alpha, an innovative online computational knowledge engine that answers factual queries directly by computing the answer from structured data. Wolfram‚Äôs theoretical work includes his explorations into the nature of complexity in the natural world, best illustrated in his book, ‚ÄúA New Kind of Science‚Äù. This work introduced the principle of computational equivalence and argued that the universe itself might be understood as a simple computational process. He recently developed this theory in the Wolfram Physics Project. Despite his contentious role in the scientific community due to his unconventional approaches, his contributions have undeniably expanded the horizon of computational science. ","date":"2023-07-09","objectID":"/posts/talks-every-developer-should-watch/:6:0","tags":null,"title":"7 Essential Tech Talks Every Developer Should Watch","uri":"/posts/talks-every-developer-should-watch/"},{"categories":null,"content":"7. Bret Victor - Inventing on Principle (2012) - It can take time to find a principle, because finding a principle is a form of self-discovery. - So if you‚Äôre guiding a principle and bodies of specific insight, it will guide you. And you will always know if what you‚Äôre doing is right. For the second time on this list, we return to Bret Victor, highlighting another one of his profound talks. The talk titled ‚ÄúInventing on Principle‚Äù presents Victor‚Äôs radical yet simple idea that creators should identify a principle that deeply resonates with them and commit their lives to making it a reality. In this talk, Victor demonstrates several examples of his principle, which is that ‚Äúcreators need an immediate connection with what they‚Äôre creating‚Äù. This belief drove Victor to develop innovative programming environments that provide real-time, tangible feedback. For instance, he demonstrates a coding environment where you can see the effects of your code changes immediately, without compiling or refreshing. This principle isn‚Äôt just about software engineering or design - it‚Äôs about living a life defined by purpose. Victor argues that finding your principle and committing your life to it isn‚Äôt just beneficial for you, but also for the world at large. When people dedicate their lives to principles that matter deeply to them, the cumulative effect can be transformative. Victor‚Äôs talk doesn‚Äôt just challenge the way we think about creating and inventing. It also asks us to reflect on our own lives and consider what principle we would commit ourselves to. In doing so, we can find direction and purpose that transcends the mundanities of daily work. As with his other talks, Victor‚Äôs unique perspective on creativity and invention is refreshing and inspiring, making this talk a must-watch for developers and creatives alike. His vision of a world where creators are guided by their principles holds exciting possibilities for the future of technology and innovation. ","date":"2023-07-09","objectID":"/posts/talks-every-developer-should-watch/:7:0","tags":null,"title":"7 Essential Tech Talks Every Developer Should Watch","uri":"/posts/talks-every-developer-should-watch/"},{"categories":null,"content":"Closing thoughts These talks epitomize the beauty of our field - it‚Äôs not about conforming to existing norms but about constantly questioning and innovating. As you explore each talk, you‚Äôll discover that these speakers have left indelible marks on programming by daring to think differently. But remember, mastery in our field isn‚Äôt confined to learning the latest languages or frameworks. It‚Äôs about cultivating a mindset of relentless curiosity and a spirit of innovation. It‚Äôs about understanding that each line of code we write is an opportunity to change the world in our own unique way. If this post resonated with you, share it with others who could benefit from these insights. Let‚Äôs continue this conversation ‚Äì I‚Äôd love to hear your thoughts in the comment section bellow on these talks or any other resources that have impacted your journey. Let‚Äôs keep pushing boundaries and transforming our field, one line of code, and one new idea, at a time. ","date":"2023-07-09","objectID":"/posts/talks-every-developer-should-watch/:8:0","tags":null,"title":"7 Essential Tech Talks Every Developer Should Watch","uri":"/posts/talks-every-developer-should-watch/"},{"categories":null,"content":"My setup for working on this blog is to write on a tablet, not my main computer, so i can‚Äôt directly run the hugo server locally. I decided to use github codespacaces for editing the blog and run it as aa dev server so I can view the edited pages after making changes. Here‚Äôs some of the links with tutorials I followed to acomplish this: post not showing - date might be wrong ","date":"2023-07-04","objectID":"/posts/hugo-edit-on-codespaces/:0:0","tags":["hugo","github","codespaces","blog"],"title":"How to work on a hugo site usign github codespaces","uri":"/posts/hugo-edit-on-codespaces/"},{"categories":null,"content":"When working with Akka Streams, you might find yourself in a situation where you need to bypass a stage in the graph. This is a good pattern if we want the whole stream to be idempotent. If we get the same message again, we want the stream to be able to process it - but only the stages that were not already processed. We usually save the extra information after each processing stage. At the start of the stream we can get the state for a message ","date":"2023-01-07","objectID":"/posts/akka-graph-dsl-bypass-pattern/:0:0","tags":["scala","cats","akka","streaming","idempotency"],"title":"Designing idempotent Streaming system","uri":"/posts/akka-graph-dsl-bypass-pattern/"},{"categories":null,"content":"Hi there! I‚Äôm a software architect / engineer, an enthusiast of many software related topics. From programming languages to architecture and design to hardware and automation. I post some of my thoughts on these topics in this blog. I live in Bucharest, Romania üá∑üá¥ . ","date":"2023-01-03","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Let‚Äôs say you have a service that runs a number of steps for it‚Äôs business process. One of those steps consumes high CPU - something to do with cryptographic functions let‚Äôs say. You run this service on 3 nodes ( you need 3 at least for high availability, no less ) You get concerned that with increasing traffic to this service the CPU starts to go above 89-90%. What are your options ? One answer comes from the much praised solution of using microservices and scale them independently. ","date":"2022-06-12","objectID":"/posts/is-independent-scaling-of-us-an-advantage/:0:0","tags":["draft-post","microservice"],"title":"Microservice scaling advantage?","uri":"/posts/is-independent-scaling-of-us-an-advantage/"},{"categories":null,"content":"If you work as a developer in a big company today, you know the agile process of having stories that you work on, and have to deliver code during the sprint. Hopefully by the end of the sprint you can also demo the working feature This style kind of assumes that you will write the code fully fleshed out in one go, and be done with it. Writing code is not a mechanistic activity where you turn clear specifications into code. There are infinity ways of implementing a feature. Writing code is more like writing poetry - you have a goal of inspiring the reader to imagine scenes and feel emotions, and you are also constrained by the rules of poetry (verses have to rhyme). It should be similar to the process of writing a book: You start out with a rough draft. When writing the draft you start changing the story to make it more believable or the characters to make them feel more authentic. After finishing the first draft, you re-read it and change it some more - take out insignificant details, change the structure of the plot-line, add supporting characters etc‚Ä¶ After you‚Äôre happy with the book you send it to a publisher to review it. They usually send feedback that you incorporate into the work. This goes back and forth until both you and the publisher are happy with the result. This is how software should be written as well - more like writing a book. We are stuck in a loop of product owners demanding features out the door as quickly as possible. This never allows us to reflect on the code, try out different designs, refactoring for a better structure. ","date":"2022-02-20","objectID":"/posts/on-writing-code/:0:0","tags":["draft-post"],"title":"On writing code","uri":"/posts/on-writing-code/"},{"categories":null,"content":" In the last post, we saw how to combine pure functions running in IO and Akka streams using .mapAsync and .unsafeToFuture val source: Source[String, NotUsed] = ??? val sink: Sink[Int, NotUsed] = ??? def pureFunction[F[_]](s: String): F[Int] = ??? source .mapAsync(parallelism = 8)(m =\u003e pureFunction[IO](m).unsafeToFuture()) .runWith(sink) In order to make it easier to work with IO in Akka Streams, we can write some helpers to add a method on Streams that automatically run the IO inside the flow. This will simplify the interaction between pure code and stream code. Readability will improve, but before we get there, it‚Äôs very important to recognise that we should view these 2 ways of writing code1 as having orthogonal purposes. ","date":"2021-02-14","objectID":"/posts/pure-functional-stream-processing-in-scala-2/:0:0","tags":["scala","cats","akka"],"title":"Pure Functional Stream processing in Scala [2]","uri":"/posts/pure-functional-stream-processing-in-scala-2/"},{"categories":null,"content":"Streams as plumbing The philosophy is identical to how Unix pipes work. You write small pieces of code using simple programs like: ps, grep, find, sed, awk, xargs, kill etc‚Ä¶ and join those smaller programs into a bigger one by piping data between them. The output of one program goes into the input of the next. There is a clear separation of duties between programs that do something2 and pipes that just pass data along between programs. Here‚Äôs an example of a pipeline that finds all java processed and stops them: ps aux | grep java | grep -v grep | awk '{ print $2 }' | xargs kill -9 The similarity between this and Akka flows is almost one to one3. One issue in Scala is that we write both the program and the pipeline / stream in the same language, which can blurry the separation of concerns‚Ä¶ For this reason, some people don‚Äôt see why we should separate these two ways of writing the full program. My view is that we should try to separate them logically to get to the simple way of composing programs like using Unix Pipelines. My suggestion here is to have separate modules for the domain, and Classes with pure functions working on the domain that run certain actions. After constructing those Classes (which can be individually unit-tested), write a Class taking the other domain and functionality Classes as constructor arguments. This Class should just combine all the individual pure functions by wrapping them in Akka flows, but nothing more. No extra business logic other than the order of operations‚Ä¶ ","date":"2021-02-14","objectID":"/posts/pure-functional-stream-processing-in-scala-2/:0:1","tags":["scala","cats","akka"],"title":"Pure Functional Stream processing in Scala [2]","uri":"/posts/pure-functional-stream-processing-in-scala-2/"},{"categories":null,"content":"Simplify Cats and Akka interaction Assuming we understand the separation of concerns between pure functions and streams, we still want to make their combination easy. Maybe as easy as the Unix pipe. The first step is to simplify using .mapAsync and IO. What we need is another method on Akka streams that automatically runs the IO4 inside the Flow by transforming it into a Future. We can enable this generically by wrapping a source or flow in extension classes with the extra method: .unsafeMapAsync implicit class SourceExtensions[A, Mat](val source: Source[A, Mat]) extends AnyVal { def unsafeMapAsync[F[_]: Effect, B](parallelism: Int)(f: A =\u003e F[B]): source.Repr[B] = source.mapAsync(parallelism)(b =\u003e Effect[F].toIO(f(b)).unsafeToFuture()) } implicit class FlowExtensions[A, B, Mat](val flow: Flow[A, B, Mat]) extends AnyVal { def unsafeMapAsync[F[_]: Effect, C](parallelism: Int)(f: B =\u003e F[C]): flow.Repr[C] = flow.mapAsync(parallelism)(b =\u003e Effect[F].toIO(f(b)).unsafeToFuture()) } The method name shows we are running the effect which is unsafe5 ‚Äì similar to the method name .unsafeToFuture This is the split between pure FP and Akka streams. We are now in streaming territory where we run the effects in each flow step. Now we can rewrite the code from the previous post. Source(List(\"1|123\", \"2|123\", \"3|789\")) .unsafeMapAsync(parallelism = 8)(m =\u003e parseMessage[IO](m)) .unsafeMapAsync(parallelism = 8)(m =\u003e getUser[IO](m.userId)) .unsafeMapAsync(parallelism = 8)(u =\u003e checkPermission[IO](u).map(p =\u003e (u, p))) .unsafeMapAsync(parallelism = 8) { case (u, p) =\u003e sendNewsletterIfAllowed[IO](u, p) } .runWith(Sink.seq) Notice the lack of calls to .unsafeToFuture() Much cleaner. Another refactor we can do is to extract individual flows in values ‚Äì same as we could do with functions, but in this case they are Flow types. val parseFlow = Flow[String].unsafeMapAsync(8)(parseMessage[IO]) val getUserFlow = Flow[Message].unsafeMapAsync(8)(m =\u003e getUser[IO](m.userId)) val checkPermissionFlow = Flow[User].unsafeMapAsync(8)(u =\u003e checkPermission[IO](u).map( (u, _) )) val sendNewsletterFlow = Flow[(User, Boolean)].unsafeMapAsync(8) { case (u, p) =\u003e sendNewsletterIfAllowed[IO](u, p) } And then use the .via method to combine them in a bigger flow val flow = parseFlow .via(getUserFlow) .via(checkPermissionFlow) .via(sendNewsletterFlow) Looks much closer to the unix pipeline example, so I‚Äôm happy üòÉ. We construct the full stream using .via to connect the Source and Sink as well Source(List(\"1|123\", \"2|123\", \"3|789\")) .via(flow) .runWith(Sink.seq) We can go a step further and use the GraphDSL val flow = Flow.fromGraph(GraphDSL.create() { implicit builder =\u003e import GraphDSL.Implicits._ val Parse = builder.add(parseFlow) val GetUser = builder.add(getUserFlow) val CheckPermission = builder.add(checkPermissionFlow) val SendNewsletter = builder.add(sendNewsletterFlow) Parse ~\u003e GetUser ~\u003e CheckPermission ~\u003e SendNewsletter FlowShape(Parse.in, SendNewsletter.out) }) Here we add the flows as nodes in the Akka Graph, then join them together in a single flow using Akka‚Äôs GraphDSL and the ~\u003e operator. The result is a graph with the Shape of a Flow ‚Äì one input, out output. We can create an actual Flow from the graph with the constructor: Flow.fromGraph Notice how similar the expression: Parse ~\u003e GetUser ~\u003e CheckPermission ~\u003e SendNewsletter Is to a unix pipeline. Just replace ~\u003e with | and it‚Äôs the same. It‚Äôs also very similar to something we might draw to represent this flow. I think this is one of the significant advantages of using Akka streams and the GraphDSL ‚Äì you can look at the actual code to see the computational graph instead of looking at representations of it. It‚Äôs not drawn by someone else, which can also become outdated quickly. The code does not lie, a representation can. The major difference between the GraphDSL and using .unsafeMapAsync directly is that we separate individual graph nodes from their connection to other nodes ‚Äì similar to the idea in Flow based pro","date":"2021-02-14","objectID":"/posts/pure-functional-stream-processing-in-scala-2/:0:2","tags":["scala","cats","akka"],"title":"Pure Functional Stream processing in Scala [2]","uri":"/posts/pure-functional-stream-processing-in-scala-2/"},{"categories":null,"content":"Complex graphs So far we haven‚Äôt looked at how to treat errors in the stream‚Ä¶ If we do nothing, for any exception thrown in the stream, Akka will simply silently ignore them‚Ä¶ There are methods to deal with that the Akka way. However, let‚Äôs look at what we can implement to make sure the stream never stops processing but we still handle errors. We can do that because we‚Äôre in control of the way we execute the effects in the stream. We can design a different stream component that will catch exceptions in the IO and push it on a separate error stream ‚Äì similar to the STDERR of Unix. To achieve this we can add another extension method on Flow and Source which will run the effect but return a component with a specific output for the Error, if there is any error thrown in the effect. implicit class FlowExtensions[A, B](val flow: Flow[A, B, NotUsed]) extends AnyVal { // ... def safeMapAsync[F[_]: Effect, C](parallelism: Int)( f: B =\u003e F[C] ): Graph[FanOutShape2[A, C, Throwable], NotUsed] = { split[A, Throwable, C]( flow.mapAsync(parallelism)(b =\u003e Effect[F].toIO(f(b)).attempt.unsafeToFuture()) ) } } We‚Äôll call this safeMapAsync since we know w‚Äôre catching exceptions as part of the graph. The logic behind is just calling .attempt on the IO, which will return an Either[Throwable, C] . We then split the Either into the 2 separate outputs of the new Graph. The return type here is no longer Flow, but it‚Äôs a Graph with the shape: FanOut with 2 outputs: Graph[FanOutShape2[A, C, Throwable], NotUsed] The input is of type A, normal output type is C and error output is Throwable. The helper method for splitting the Either is a bit more verbose‚Ä¶ but it‚Äôs very generic, so you only have to write it once. The helper method for splitting the Either is a bit more verbose‚Ä¶ but it‚Äôs very generic, so you only have to write it once. def split[A, E, B](flow: Flow[A, Either[E, B], NotUsed]): Graph[FanOutShape2[A, B, E], NotUsed] = GraphDSL .create() { implicit builder =\u003e import GraphDSL.Implicits._ val F = builder.add(flow) val B = builder.add(Broadcast[Either[E, B]](2)) val LeftBranch = builder.add(Flow[Either[E, B]].collect { case Left(b) =\u003e b }) val RightBranch = builder.add(Flow[Either[E, B]].collect { case Right(c) =\u003e c }) F ~\u003e B ~\u003e LeftBranch B ~\u003e RightBranch new FanOutShape2(F.in, RightBranch.out, LeftBranch.out) } Now we can again rewrite the original stream to capture the error outputs and merge them into a generic error handling flow. First, we need to update the components to use .safeMapAsync. val parseComponent = Flow[String].safeMapAsync(8)(parseMessage[IO]) val getUserComponent = Flow[Message].safeMapAsync(8)(m =\u003e getUser[IO](m.userId)) val checkPermissionComponent = Flow[User].safeMapAsync(8)(u =\u003e checkPermission[IO](u).map( (u, _) )) val sendNewsletterComponent = Flow[(User, Boolean)].safeMapAsync(8) { case (u, p) =\u003e sendNewsletterIfAllowed[IO](u, p) } We then write the graph using these components GraphDSL.create() { implicit builder =\u003e import GraphDSL.Implicits._ val (parse, parseErr) = toTuple(builder.add(parseComponent)) val (getUser, getUserErr) = toTuple(builder.add(getUserComponent)) val (checkPermission, checkPermissionErr) = toTuple(builder.add(checkPermissionComponent)) val (sendNewsletter, sendNewsletterErr) = toTuple(builder.add(sendNewsletterComponent)) val (handleErrors, handleErrorsErr) = toTuple(builder.add(handleErrorsComponent)) val M = builder.add(Merge[Throwable](5)) // format: off parse ~\u003e getUser ~\u003e checkPermission ~\u003e sendNewsletter parseErr ~\u003e M getUserErr ~\u003e M checkPermissionErr ~\u003e M sendNewsletterErr ~\u003e M ~\u003e handleErrors M \u003c~ handleErrorsErr // format: on new FanOutShape2[String, Unit, Throwable](parse.in, sendNewsletter.out, handleErrors.out) } The toTuple6 helper just recreates a Flow for the success path and a separate Source with the errors from the FanOutShape2 graph to make it easier to connect. If we look at this drawing of this graph and compare it with the code, we‚Äôll see that they are quite similar Complex C","date":"2021-02-14","objectID":"/posts/pure-functional-stream-processing-in-scala-2/:0:3","tags":["scala","cats","akka"],"title":"Pure Functional Stream processing in Scala [2]","uri":"/posts/pure-functional-stream-processing-in-scala-2/"},{"categories":null,"content":"Conclusion In this post, we find how to create Akka flows from pure functions in a syntactically cleaner way, with the help of some extension classes over Source and Flow. We also see how to construct more complex components and stitch them together using the GraphDSL. In future posts, I‚Äôll explore more patterns of constructing and combining components. Coments thread on Reddit Pure code and Streams code¬†‚Ü©Ô∏é Programs that do some work and have side effects¬†‚Ü©Ô∏é The difference is that Unix pipes have 2 output streams: STDOUT and STDERR. In Akka flows, you only have one output as a stream ‚Äì which is for successful processing. It throws errors out of bound ‚Äì you don‚Äôt have an error stream¬†‚Ü©Ô∏é We can be more generic by using the Effect type constraint instead of IO directly¬†‚Ü©Ô∏é Because it executes side effects¬†‚Ü©Ô∏é def toTuple[I, O, E](g: FanOutShape2[I, O, E]): (FlowShape[I, O], Outlet[E]) = { val Err = g.out1 val Success = g.out0 (FlowShape(g.in, Success), Err) } ¬†‚Ü©Ô∏é which could lead to unfortunate things like messages stuck in an infinite loop, hogging CPU and memory¬†‚Ü©Ô∏é making sure to handle errors in the loop, so they don‚Äôt cycle infinitely¬†‚Ü©Ô∏é ","date":"2021-02-14","objectID":"/posts/pure-functional-stream-processing-in-scala-2/:0:4","tags":["scala","cats","akka"],"title":"Pure Functional Stream processing in Scala [2]","uri":"/posts/pure-functional-stream-processing-in-scala-2/"},{"categories":null,"content":"In Scala you can write pure functional code, similar to Haskell or other pure functional languages, but you‚Äôre not obligated to. Wikipedia categories Scala as an impure Functional language. FP purists view this as a weakness of Scala, but others view the option of ‚Äúcheating‚Äù pureness as an acceptable choice sometimes. Even if you can do everything purely, it‚Äôs sometimes a lot easier to think about the problem in a different paradigm. Pure FP is great for writing correct functions you can easily reason about in isolation and compose well with other pure functions. We can easily unit test them since pure functions only depend on their input arguments and always produce the same output for the same arguments ‚Äì they are referentially transparent. This allows the programmer and the compiler to reason about program behavior as a rewrite system. This can help in proving correctness, simplifying an algorithm, help change code without breaking it, or optimizing code through memoization, common sub-expression elimination, lazy evaluation, or parallelization. There are, however, other approaches to thinking about compossibility of programs. One such approach is to think of software components as black boxes running a process. They have a variable number of input and output ports which have Types associated to them. Messages pass asynchronously from component to component after linking their corresponding ports together (if the types match). We specify the connections outside the components themselves. This is the thinking behind flow based programming. (This is also how microservices work at a larger scale) My view is that these two ways of thinking about composable programs are not mutually exclusive and they can work together in synergy. I will try to make this case by the end of this post. ","date":"2021-02-06","objectID":"/posts/pure-functional-stream-processing-in-scala-1/:0:0","tags":["scala","cats","akka"],"title":"Pure Functional Stream processing in Scala [1]","uri":"/posts/pure-functional-stream-processing-in-scala-1/"},{"categories":null,"content":"Pure Functional Programming in Scala Using Cats, you can use Type Classes: Functor, Applicative, Monad etc‚Ä¶ to model your programs based on these highly general computational abstractions. There are other ecosystems for pure FP in Scala. I chose Cats because I‚Äôm most familiar with it. Adding Cats-effect, you can also model IO in a pure functional way. The idea is to write the entire program, including all the effects like: calling external services, writing to file, pushing messages to queues, as a single composed expression that returns an IO data structure representing the action of running all these effects, without actually running them. You only execute them at the ‚Äúend of the world‚Äù in the ‚Äúmain‚Äù method. This is a simple example of a pure functional program using cats-effect. import cats.effect.{ IO, Sync } import cats.implicits._ import scala.io.StdIn object App { def getUserName[F[_]: Sync]: F[String] = for { _ \u003c- Sync[F].delay(println(\"What's your name?\")) name \u003c- Sync[F].delay(StdIn.readLine()) } yield name def greatUser[F[_]: Sync](name: String): F[Unit] = Sync[F].delay(println(s\"Hello $name!\")) def program[F[_]: Sync]: F[String] = for { name \u003c- getUserName _ \u003c- greatUser(name) } yield s\"Greeted $name\" def main(args: Array[String]): Unit = { program[IO].unsafeRunSync() } } Real programs will, of course, be much more complex, but it all boils down to a single IO value that combines all the effects of running the program which we execute in the ‚Äúmain‚Äù method. Runar has a great talk where he compares using pure FP and IO as working with unexploded TNT. That is much easier to work with as opposed to working with exploded TNT (by actually executing effects in each function). ","date":"2021-02-06","objectID":"/posts/pure-functional-stream-processing-in-scala-1/:0:1","tags":["scala","cats","akka"],"title":"Pure Functional Stream processing in Scala [1]","uri":"/posts/pure-functional-stream-processing-in-scala-1/"},{"categories":null,"content":"Stream processing in Scala Akka Streams implements the Reactive Streams protocol that‚Äôs now standardised in the JVM ecosystem. Streams have added benefits over simple functions by implementing flow control mechanisms which include back-pressure. You can think of streams as managed functions, similar to how the Operating System manages threads. A stream component can decide when to ask for more input messages to pass to its processing function, how many parallel calls to the function to allow, and whether to slow down processing because there is no demand from downstream functions. Akka is using the abstractions of Source, Flow, Sink for modeling streams. Source via Flow to Sink A Source is a Component that has one output port and generates elements of some type. A Sink is a Component that has a single input port that consumes elements of some type. A Flow is a combination of a Sink and a Source having a single input and single output port. We can think of a Flow as a processor transforming a message into another. A big difference from normal functions is that the number of input messages does not have to match the number of output messages‚Ä¶ For instance, a Flow can consume 1 input messages but produce 2 output messages (or none). In that sense, Flows differs greatly from functions ‚Äì which will always produce an object of its return type given input arguments (assuming it throws no exceptions. For pure functions this should hold). Those are the simplest components, but we can write more complex ones. For instance ‚Äì components having a single input port but two output ports. I like to model failure in components like this: I use the first output port as the normal (success) output, and the second one as the error output. For a single input, only one of the output ports yield a message. Component with explicit Error output What I like about Akka Streams is the ability to compose components using the Graph DSL. The way you write code can be very intuitive, as it‚Äôs almost a one-to-one match with drawing boxes and links between them. Architects and devs are quite familiar with this way of thinking, so it seems very natural. Here‚Äôs an example of using Graph DSL to write a complex stream. (The code and graph image representation are from the Akka Streams documentation) import GraphDSL.Implicits._ RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =\u003e val A: Outlet[Int] = builder.add(Source.single(0)).out val B: UniformFanOutShape[Int, Int] = builder.add(Broadcast[Int](2)) val C: UniformFanInShape[Int, Int] = builder.add(Merge[Int](2)) val D: FlowShape[Int, Int] = builder.add(Flow[Int].map(_ + 1)) val E: UniformFanOutShape[Int, Int] = builder.add(Balance[Int](2)) val F: UniformFanInShape[Int, Int] = builder.add(Merge[Int](2)) val G: Inlet[Any] = builder.add(Sink.foreach(println)).in C \u003c~ F A ~\u003e B ~\u003e C ~\u003e F B ~\u003e D ~\u003e E ~\u003e F E ~\u003e G ClosedShape }) Graph representation Other stream processing frameworks in Scala like fs2 don‚Äôt have such DSL‚Äôs, and it may be difficult to write certain kinds of flows. For instance, if you have a loop from a downstream component to an upstream one, we can easily model this with Akka Graph DSL but not as easily with fs2. ","date":"2021-02-06","objectID":"/posts/pure-functional-stream-processing-in-scala-1/:0:2","tags":["scala","cats","akka"],"title":"Pure Functional Stream processing in Scala [1]","uri":"/posts/pure-functional-stream-processing-in-scala-1/"},{"categories":null,"content":"Combining Cats and Akka The Idea is simple: use pure functions to write all the business code and effects (like external system calls, db calls etc‚Ä¶) in IO using Cats and Cats Effect. I‚Äôll add some Case Classes and mocked functions so we have something to work with in the examples below: def parseMessage[F[_]: Sync](msg: String): F[Message] = Sync[F] .delay(msg.split('|')) .map(split =\u003e Message(id = split(0), userId = split(1))) def getUser[F[_]: Sync](userId: String): F[User] = if (userId == \"123\") Sync[F].pure(User(userId, \"Mihai\", \"me@mihaisafta.com\")) else Sync[F].pure(User(userId, \"John\", \"John@example.com\")) def checkPermission[F[_]: Sync](user: User): F[Boolean] = user.name match { case \"Mihai\" =\u003e Sync[F].pure(true) case _ =\u003e Sync[F].pure(false) } def sendNewsletter[F[_]: Sync](user: User): F[Unit] = for { nl \u003c- Sync[F].pure(NewsLetter(user.name, \"Hello, this is the newsletter\")) _ \u003c- Sync[F].delay(println(s\"Sent newsletter: $nl to user $user\")) } yield () def sendNewsletterIfAllowed[F[_]: Sync](user: User, userPermission: Boolean): F[Unit] = if (userPermission) sendNewsletter(user) else Sync[F].delay(println(s\"Not sending newsletter to $user\")) We could write a pure FP program to run these steps sequentially def program[F[_]: Sync](msg: String): F[Unit] = for { message \u003c- parseMessage(msg) user \u003c- getUser(message.userId) allowsEmails \u003c- checkPermission(user) _ \u003c- sendNewsletterIfAllowed(user, allowsEmails) } yield () But now let‚Äôs see how we could wrap each step using Akka Streams. Since Akka Streams can‚Äôt work in a purely functional style, we have to execute the effects of the operation in each part of the flow‚Ä¶ we can‚Äôt remain in the pure functional programing style of constructing a single IO and running it only once. We are moving from the pure FP world into the Flow Based Programming World. Source(List(\"1|123\", \"2|123\", \"3|789\")) .mapAsync(parallelism = 8)(m =\u003e parseMessage[IO](m).unsafeToFuture()) .mapAsync(parallelism = 8)(m =\u003e getUser[IO](m.userId).unsafeToFuture()) .mapAsync(parallelism = 8)(u =\u003e checkPermission[IO](u).map(p =\u003e (u, p)).unsafeToFuture()) .mapAsync(parallelism = 8) { case (u, p) =\u003e sendNewsletterIfAllowed[IO](u, p) } .runWith(Sink.seq) Notice that we have to call the .unsafeToFuture() method in each .mapAsyncUnordered step. I think of it like each step in the stream is a ‚Äúmain‚Äù program itself. Just like how the OS manages your processes and threads, here the stream manages your pure functions. We only use the stream for wrapping functions, sending messages through the sequence of wrapped functions and applying flow control. Not for domain logic, which we write only in pure functions. Using parallelism set to 8 means that each step can process 8 messages in parallel without blocking the upstream steps. If the ‚ÄúsendNewsletter‚Äù call is slow, it will create back-pressure up the chain until it reaches the Source which will stop sending more messages until there is demand again. Back-pressure is a key benefit of streaming. It guarantees that fast producers don‚Äôt overwhelm slow consumers, which could lead to issues from degraded performance to crashes caused by out of memory exceptions. Without streaming technology, you need to implement back-pressure yourself to get the same guarantees‚Ä¶ Pure FP or even Actor based systems don‚Äôt have that built in. ","date":"2021-02-06","objectID":"/posts/pure-functional-stream-processing-in-scala-1/:0:3","tags":["scala","cats","akka"],"title":"Pure Functional Stream processing in Scala [1]","uri":"/posts/pure-functional-stream-processing-in-scala-1/"},{"categories":null,"content":"Conclusion This is the first part in a series of post where I explore how to connect pure functional programming using Cats and Streaming with Akka Streams. So far we‚Äôve seen that you can write pure functions and embed them in Akka streams using .mapAsync. You also have to run the effects inside each step using .unsafeToFuture In the next parts we‚Äôll see how to simplify the interaction between pure functions and Akka stream ‚Äì by abstracting the need to call .unsafeToFuture every time. Also, we‚Äôll construct more complex component and use the Akka GraphDSL to combine them in interesting ways. Hope this helps üòÄ Please leave comments or suggestions on this Reddit thread, and get in touch with me on Twitter. ","date":"2021-02-06","objectID":"/posts/pure-functional-stream-processing-in-scala-1/:0:4","tags":["scala","cats","akka"],"title":"Pure Functional Stream processing in Scala [1]","uri":"/posts/pure-functional-stream-processing-in-scala-1/"},{"categories":null,"content":"Theory Having complete silence while you sleep üò¥ may seem nice, but in fact can lead to trouble if the outside environment is not also completely quiet. A cat may meowüê±, a car may honk üöó, your upstairs neighbour can wake up for a late-night snack üçü‚Ä¶ All these sounds will seem louder if your room is completely quiet. The human perception of sound is nonlinear. That‚Äôs why it‚Äôs measured in decibels. We describe a 10 times increase in volume as a 10 points increase in the decibel measure. Also, the brain is very adept at filtering out constant noise. After a short acclimation period, the brain will consider the noise to be silence. They use this trick in horror movies. They place a constant white noise in the background and just before the jump scare, they quiet down that noise also, which makes the silence seem more silent üòà That‚Äôs the logic behind using white noise for sleeping: we intentionally add a background noise for our brains to filter out. Then, if there are outside noises are below the db level of our background white noise, we basically won‚Äôt hear them anymore and can sleep soundly without interruptions üò¥. ","date":"2021-01-31","objectID":"/posts/rpi-white-noise/:0:1","tags":["automation","self-host"],"title":"Sleep better with Raspberry Pi automated white noise","uri":"/posts/rpi-white-noise/"},{"categories":null,"content":"Implementation Connect your Pi to some speakers Raspberry py white noise Write a script: /home/pi/bin/whitenose.sh It will call omxplayer to start your preferred white noise sound. I used a free sample Airplane Cabin noise I found on the internet (10 hours long mp3, 1.4 GB) and added it to the /media directory of the Pi. #!/usr/bin/env bash nohup omxplayer /media/Airpane_Cabin_White_Noise.mp3 \u0026\u003e ~/whitenoise.log Don‚Äôt forget to make it executable: chmod +x /home/pi/bin/whitenose.sh Then install the script in the Raspberry‚Äôs cron at your desired hour to go to sleep, in my case it‚Äôs 23:00. 0 23 * * * /home/pi/bin/whitenoise.sh \u0026 That‚Äôs it‚Ä¶ Every evening at 23:00 the sound will start. No need for white noise apps, no subscriptions, no asking Google Home to start white noise every evening, no internet connection required. Just a cheap Raspbery Pi, some old speakers and a bit of linux knowledge. Huzzah! Of course this can work on any old Linux machine you have lying around. Hope this helps üòÄ Sleep well! Edit: Discussion link on Reddit: selfhosted ","date":"2021-01-31","objectID":"/posts/rpi-white-noise/:0:2","tags":["automation","self-host"],"title":"Sleep better with Raspberry Pi automated white noise","uri":"/posts/rpi-white-noise/"}]